---
title: "\\textbf{Resume Data: Influence of Gender and Race on Job Application Callbacks}"
subtitle: "\\textbf{Data Analysis Assignment 02 - Resume Data}"
format:
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
editor: visual
geometry:
     - top=10mm
     - left=12mm
     - right=12mm
header-includes:
   - "\\usepackage{booktabs}"
   - "\\usepackage{titling}"
author: "Divya Sharma (ds655)"
---

```{r,include=FALSE}
#Importing Libraries
library(dplyr, quietly = T)
library(caret, quietly = T)
library(irr, quietly = T)
library(pROC, quietly = T)
library(psych, quietly = T)
library(stargazer, quietly = T)
library(gridExtra, quietly=T)

#Reading the data
library(openintro, quietly = T)
data("resume")

#Storing it in a dataframe called 'base_data'
base_data <- resume
# head(resume)
# print(nrow(base_data))
# print(ncol(base_data))
#glimpse(base_data)
```

# Model Documentation

*Data Source and Dictionary:* [OpenIntro](https://www.openintro.org/data/index.php?data=resume)

This experiment data comes from a study that sought to understand the influence of race and gender on job application callback rates. The study monitored job postings in Boston and Chicago for several months during 2001 and 2002 and used this to build up a set of test cases. Over this time period, the researchers randomly generating resumes to go out to a job posting, such as years of experience and education details, to create a realistic-looking resume. They then randomly assigned a name to the resume that would communicate the applicant's gender and race.

**Research Question:** How do race and gender influence job application callback rates?

### 1. Overview

The resume data in the OpenIntro Library is a dataset of Resumes that were used to apply for job profiles, and whether or not they recieved a callback. The resume dataset contains the following fields -

-   *Job Details* - These include details such as City, Industry, Job Title, Private/Non Profit, required education, and required skills
-   *Applicant Details* - Details about the applicant, such as Gender, Race, years of education, college degree, skills, and years of experience
-   *Resume Details* - Details about the resume, such Email available, Resume Quality
-   *Callback* - whether the applicant received a call back for this job posting for their resume (1 or 0) - this will be the *dependent variable*

The source data contains 4,870 rows and 30 columns. Out of 4,870 job-resume combinations, 392 received a callback. The dataset will be used to train a logistic regression model to predict the probability of receiving an interview invite, given the gender and socioeconomic class of the applicant.

### 2. Data Cleaning and EDA

#### 2.1: Missing Values

-   The `job_req_min_experience` column contains 2,746 NULL values - this is 56% missing information, so we are not going to use this column for the model
-   The `job_fed_contractor` column has 1,768 (36%) NAs, this can also be excluded
-   The `job_ownership` column has 1,992 (40%) unknowns

#### 2.2: Data Cleaning

For variables that are stored as numeric 0 and 1 but are actually flags (computer_skills, job_req_any etc) - converting them to factors before feeding this to the model. The variables include -

-   `gender` - Gender (male or Female)
-   `resume_quality` - Resume Quality (high or low)
-   `race` - Race (black or white)
-   `job_equal_opp_employer` - Whether the employer is an equal opportunity employer (0 or 1)
-   `job_fed_contractor` - Whether employer is a federal contractor (0 or 1)
-   `job_req_any` - Whether job has any requirements (0 or 1)
-   `job_req_communication` - Whether job requires communication skills (0 or 1)
-   `job_req_education` - Whether job requires education (0 or 1)
-   `job_req_computer` - Whether job requires computer skills (0 or 1)
-   `job_req_organization` - Whether job requires organization skills (0 or 1)
-   `honors` - Whether applicant has honors (0 or 1)
-   `worked_during_school` - Whether applicant worked during school (0 or 1)
-   `computer_skills` - Whether applicant has computer skills (0 or 1)
-   `special_skills` - Whether applicant has special skills (0 or 1)
-   `volunteer` - Whether applicant is a volunteer (0 or 1)
-   `military` - Whether applicant was in the military (0 or 1)
-   `employment_holes` - Whether applicant has any gaps in employment (0 or 1)
-   `has_email_address` - Whether resume has an email address (0 or 1)


```{r, include=FALSE}
#Converting factor variables to factors, filling missing data in job_req_min_experience 
resume$job_req_min_experience <- ifelse(resume$job_req_min_experience == "", 0, resume$job_req_min_experience)
resume$job_req_min_experience <- ifelse(resume$job_req_min_experience == "some", 0.5, resume$job_req_min_experience)

resume$gender_factors <- factor(resume$gender, levels = c("m", "f"))
resume$resume_quality_factors <- factor(resume$resume_quality, levels = c("low", "high"))
resume$race_factors <- factor(resume$race, levels = c("black", "white"))
resume$job_equal_opp_employer <- as.factor(resume$job_equal_opp_employer)
resume$job_fed_contractor <- as.factor(resume$job_fed_contractor)
resume$job_req_any <- as.factor(resume$job_req_any)
resume$job_req_communication <- as.factor(resume$job_req_communication)
resume$job_req_education <- as.factor(resume$job_req_education)
resume$job_req_min_experience <- as.factor(resume$job_req_min_experience)
resume$job_req_computer <- as.factor(resume$job_req_computer)
resume$job_req_organization <- as.factor(resume$job_req_organization)
resume$honors <- as.factor(resume$honors)
resume$worked_during_school <- as.factor(resume$worked_during_school)
resume$computer_skills <- as.factor(resume$computer_skills)
resume$special_skills <- as.factor(resume$special_skills)
resume$volunteer <- as.factor(resume$volunteer)
resume$military <- as.factor(resume$military)
resume$employment_holes <- as.factor(resume$employment_holes)
resume$has_email_address <- as.factor(resume$has_email_address)
```

#### 2.3: Exploratory Data Analysis (EDA)

EDA is done to examine the correlations between the predictor variables such as gender, race, resume details etc and the outcome variable which is received callback.

We observe that in gender, females received higher callbacks compared to males(309 vs 83), however there were a lot more applications by females as compared to males (3746 vs 1124). Overall, females received callbacks `8.25%` times which is higher than males (`7.38%`)

While looking at race, we observed that there were equal applications for black and white people, however, black people got callbacks `6.45%` times which is much much lower than that of white people (`9.65%`) times.

### 3. Modeling

We will be using this data to predict whether or not a callback was received, based on the provided data of job details, applicant details, and resume quality. This is an inference problem, so we are more interested in what variables are significant towards receiving a callback, rather than the accuracy of the model.

*One major issue that we can face in this model is that of class imbalance, as only 392 out of 4,870 `(~8%)` job-resume combinations got a callback*

Currently, Logistic Regression is a good choice for this problem due to a variety of reasons -

-   Logistic Regression is a powerful tool for modeling the probability of a binary outcome
-   It can be used to account for the effects of multiple independent variables on the outcome variable
-   It is easier to interpret and explain to stakeholders
```{r, include=FALSE}
#|echo: false
set.seed(9482)
sample <- sample(c(TRUE, FALSE), nrow(resume), replace=TRUE, prob=c(0.8,0.2))
train <- resume[sample,]
test <- resume[!sample,]

model <- glm(received_callback~
               job_city+
               job_industry+
               job_type+
               #job_req_min_experience+
               #resume_quality_factors+
               gender_factors+
               race_factors+
               #years_college+
               #college_degree+
               honors+
               #worked_during_school+
               years_experience+
               computer_skills+
               #volunteer+
               #military+
               employment_holes
               #has_email_address
             , family="binomial", data=resume)
options(scipen=999)
summary(model)

resume$test_results <- predict(model, resume, type = 'response')
#table_mat <- table(resume$received_callback, test_results > 0.2)
#table_mat

#precision <- function(matrix) {
	# True positive
#    tp <- matrix[2, 2]
	# false positive
#    fp <- matrix[1, 2]
#    return (tp / (tp + fp))
#}
#print(precision(table_mat))
#recall <- function(matrix) {
# true positive
#    tp <- matrix[2, 2]# false positive
#    fn <- matrix[2, 1]
#    return (tp / (tp + fn))
#}
#print(recall(table_mat))


threshold <- 0.2
confusionMatrix(factor(resume$test_results>threshold), factor(resume$received_callback==1), positive="TRUE")
#kappa2(resume[c('received_callback', 'test_results')])
```
#### Table 3.1: Model Output: Coefficients
```{r echo=FALSE}
# Use the new object in the stargazer function
stargazer(model, 
          title = "Logistic Regression Results - Coefficient + Confidence Intervals @ 97.5% for variables", 
          type = "text",
          dep.var.labels=c("Callback Received"),
          float = TRUE, single.row = TRUE,
          align=TRUE,
          ci = TRUE, ci.level = 0.98,
          covariate.labels=c(
            "Job City: Chicago",
            "Job Industry: Finance/Insurance/Real Estate",
            "Job Industry: Manufacturing",
            "Job Industry: Other Service",
            "Job Industry: Transportation/Communication",
            "Job Industry: Wholesale and Retail Trade",
            "Job Type: Manager",
            "Job Type: Retail Sales",
            "Job Type: Sales Rep",
            "Job Type: Secretary",
            "Job Type: Supervisor",
            "Gender: F",
            "Race: White",
            "Has Honors: True",
            "Has Years of experience: True",
            "Has Computer Skills: True",
            "Has Employment Holes: True",
            "Constant"
          ),
          no.space = TRUE)

```

### 4. Results

Now that we have built a logistic regression model, we can assess the performance using the following metrics

#### 4.1 Assessing Model Performance - APR Metrics
We can observe model performance metrics such as Accuracy, Precision, Recall, Sensitivity, F1 Score, Kappa Score etc
```{r, echo=FALSE}
# Converting the predicted probabilities to binary predictions
resume$predicted_classes <- factor(ifelse(resume$test_results > 0.15, 1, 0), levels = c(0, 1))
resume$actual_classes <- factor(ifelse(resume$received_callback == 1, 1, 0), levels = c(0, 1))
```
##### Fig 4.1: Model Output Metrics
```{r, echo=FALSE, warning=FALSE}
library(tibble)
# Calculate the confusion matrix and performance metrics
confusion_matrix <- confusionMatrix(resume$predicted_classes, resume$actual_classes)

#print(confusion_matrix$table, digits = 2)
#as.matrix(confusion_matrix, what = "classes")
#as.matrix(confusion_matrix, what = "overall")

ndf <- as.data.frame(rbind(
    as.matrix(confusion_matrix, what = "classes"), 
    as.matrix(confusion_matrix, what = "overall"))
  )
names(ndf) <- c('Value')
ndf <- rownames_to_column(ndf, var = 'Metric')
stargazer(ndf,type='text', summary=FALSE, colnames=TRUE)

```

-   `Accuracy`: A model with an accuracy of **0.87** predicts the correct outcome **87.6%** of the time. *Note that Accuracy is not a good measure of model performance due to class imbalance*
-   `Precision`: A precision of **0.92** predicts the positive outcome correctly **92.7%** of the time when it predicts a positive outcome.
-   `Recall`: A model with a recall of **0.93** correctly identifies **93%** of the positive cases.
-   `Kappa`: A model with a kappa of **0.11** has a fair agreement between the predicted and actual outcomes, after accounting for the possibility of agreement occurring by chance.

### 4.2 Assessing Model Performance - ROC Curve

\centering
#### Fig 4.2: ROC AUC Curve for GLM Model
```{r, echo=FALSE,fig.width = 4,fig.height = 3, warning=FALSE, message=FALSE}
# Calculating the ROC curve
roc_curve <- roc(resume$received_callback, resume$test_results)
# Plotting the ROC curve
t <- plot(roc_curve, main = "ROC Curve for Model with AUC", print.auc = TRUE)
```

\raggedright

An ROC of \> 0.5 means that the model is better at predicting than chance. An ROC of 0.642 indicates that the model is able to predict the probability of a callback with reasonable accuracy.

### 4.3 Interpreting the Results

We observed Race to be a significant factor in influencing callback rates, however Gender does not have a very high impact on influencing callback rates. 9.7% white people got a callback compared to 6.5% black people, while 8.3% females got callbacks as compared to 7.4% males 

#### Fig 4.3: Relationship between Gender and Race vs Received Callback
\centering
```{r, echo=FALSE,fig.width = 7, fig.height = 2.5}

p1 <- ggplot(resume, aes(x = race, fill = factor(received_callback))) +
  geom_bar() +
  labs(title = "Fig 4.3.1: Race and Callback", x = "Race", y = "#Calls") +scale_y_continuous(name = "#Calls", labels = scales::comma) +
  theme(legend.position = "bottom")+
  scale_fill_discrete(name = "Callback", labels = c("Not Received", "Received"))+
  geom_text(stat = "count", aes(label = scales::comma(after_stat(count))), position = position_stack(vjust = 0.5))

p2 <- ggplot(resume, aes(x = gender_factors, fill = factor(received_callback))) +
  geom_bar() +
  labs(title = "Fig 4.3.2: Gender and Callback", x = "Gender", y = "#Calls") +
  scale_y_continuous(name = "#Calls", labels = scales::comma) +
  theme(legend.position = "bottom")+
scale_fill_discrete(name = "Callback", labels = c("Not Received", "Received"))+  geom_text(stat = "count", aes(label = scales::comma(after_stat(count))), position = position_stack(vjust = 0.5))

na <- grid.arrange(p1, p2, ncol = 2, top = "", bottom = "") #+
  #theme(plot.margin = unit(c(1, 1, 2, 1), "lines"))
```
\raggedright

## 5. Future Work

While the model can infer the most significant factors that resulted in recieving a callback, moving forward we can fix the class imbalance issue by using sampling methods. Once there is a better ratio of callbacks to non-callback applications, we can feed that data to the model to improve the performance exponentially.

```{r include=FALSE}
library(PRROC)
pr_auc <- pr.curve(scores.class0 = resume$test_results, weights.class0 = 1 - resume$received_callback, curve = TRUE)$auc.integral
pr_auc
plot(pr.curve(scores.class0 = resume$test_results, weights.class0 = 1 - resume$received_callback, curve = TRUE))
```
